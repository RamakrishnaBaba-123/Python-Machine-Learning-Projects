{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "02-CharacterLevel LSTM_Practical.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamakrishnaBaba-123/Python-Machine-Learning-Projects/blob/main/02_CharacterLevel_LSTM_Practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAGUqDTzw9Up"
      },
      "source": [
        "<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"100\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTCLJLt0w9Us"
      },
      "source": [
        "<center><h1>Building Character Level Recurrent Neural Networks<center/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHqqzEjJajvw"
      },
      "source": [
        "---\n",
        "# **Table of Contents**\n",
        "---\n",
        "\n",
        "**1.** [**Introduction to Character Level Language Modelling**](#Section1)<br>\n",
        "**2.** [**Problem Description**](#Section2)<br>\n",
        "**3.** [**Installing & Importing Libraries**](#Section3)<br>\n",
        "**4.** [**Data Acquisition & Description**](#Section4)<br>\n",
        "**5.** [**Data Preprocessing**](#Section5)<br>\n",
        "**6.** [**Character Level LSTM Model**](#Section6)<br>\n",
        "  - **6.1** [**Define LSTM Model**](#Section61)\n",
        "  - **6.2** [**Model Training**](#Section62) \n",
        "  - **6.3** [**Model Testing**](#Section63)\n",
        "  - **6.4** [**Building Larger Model**](#Section61)\n",
        "  - **6.5** [**Text Generation**](#Section62)\n",
        "\n",
        "**7.** [**Conclusion**](#Section8)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jakTEOLgw9Uu"
      },
      "source": [
        "---\n",
        "<a name = Section1></a>\n",
        "# **1. Introduction to Character Level Language Modelling**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpWrizeTKwcE"
      },
      "source": [
        "- When modeling the **joint distribution** of a sentence, a simple **n-gram model** would give zero **probability** to all of the combination that were not encountered in the training corpus.\n",
        "\n",
        "- i.e. It would most **likely** give zero **probability** to most of the **out-of-sample** test cases. However, new **combinations** of n words that were not seen in the **training** set are likely to occur, thus we do not want to **assign** such cases zero **probability**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_m-is-sw9Uv"
      },
      "source": [
        "- A **language model** is a particular kind of **machine learning algorithm** that learns the statistical structure of language by **\"reading\" a large corpus** of text. \n",
        "\n",
        "- This model can then produce authentic **language segments** by **predicting the next character** (or word, for word-based models) based on **past characters.**\n",
        "\n",
        "- **Simpler models** may look at a context of a **short sequence** of words, whereas larger **models** may work at the level of **sentences or paragraphs**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-V-hz2kw9Uy"
      },
      "source": [
        "<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/google-hangouts-feature.png\"width=\"600\" height=\"250\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51fvf6SMw9Uz"
      },
      "source": [
        "### **Word Level Language Modelling**:\n",
        "\n",
        "<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/worlevelpro.JPG\"width=\"400\" height=\"200\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_9j87w-EUN0"
      },
      "source": [
        "---\n",
        "<a name = Section2></a>\n",
        "# **2. Problem Statement**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8PFJvEgw9Ux"
      },
      "source": [
        "- The main task of the **character-level** language model is to **predict** the next character given all **previous characters** in a sequence of data, i.e. generates **text character** by character.\n",
        "\n",
        "- The **character-based part** of the model's name means that **every input vector** represents a **single character** (as opposed to, say, a word or part of an image).\n",
        "\n",
        "- The models are **prepared** for the prediction of **words** by learning the **features** and **characteristics** of a language.\n",
        "\n",
        "- A language model **learns** the probability of **word occurrence** based on **examples** of text. \n",
        "\n",
        "<center><img src = \"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/char-lstm.jpg\"width=\"800\" height=\"210\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOmVgY8yWE36"
      },
      "source": [
        "---\n",
        "<a name = Section3></a>\n",
        "# **3. Installing and Importing Libraries**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItkIWd-QuyJn"
      },
      "source": [
        "# Import tensorflow 2.x\n",
        "# This code block will only work in Google Colab.\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We__8eOQw9U3"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9OWuHO0WF1n"
      },
      "source": [
        "---\n",
        "<a name = Section4></a>\n",
        "# **4. Data Acquisition & Description**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REB21MaBw9U1"
      },
      "source": [
        " - We are going to use a favorite book from **childhood** as the dataset:\n",
        "\n",
        "   - **Alice’s Adventures in Wonderland** by Lewis Carroll.\n",
        "\n",
        "<br>  \n",
        " <center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/51Dp6aAR4HL._SX357_BO1%2C204%2C203%2C200_.jpg\" width=\"300\" height=\"450\"/></center>\n",
        "\n",
        "<br>  \n",
        " - We are going to learn the **dependencies** between **characters** and the conditional **probabilities** of characters in **sequences** so that we can in turn generate **whole** new and original **sequences** of characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLfSw_jcw9U8"
      },
      "source": [
        "import urllib\n",
        "sample = urllib.request.urlopen('https://raw.githubusercontent.com/insaid2018/DeepLearning/master/Data/Alice.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lszB-fmkWW4f"
      },
      "source": [
        "---\n",
        "<a name = Section5></a>\n",
        "# **5. Data Preprocessing**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYbXtzephS7U"
      },
      "source": [
        "raw_text = sample.read().decode('utf8')\n",
        "# Converting all text to lower case.\n",
        "raw_text = raw_text.lower()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA0zaWlVw9VB"
      },
      "source": [
        "**Remove Preface of the Book**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN147iDgw9VC"
      },
      "source": [
        "raw_text = raw_text[623:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bABQF43Bw9VF"
      },
      "source": [
        "**Remove Liscence that is present at the end of the Book.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veDLjjLaw9VG"
      },
      "source": [
        "raw_text = raw_text[0:-18757]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EjVAj7Bw9VI"
      },
      "source": [
        "**Sample Page from BOOK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bqCp0DCw9VI",
        "outputId": "cbc61c22-7654-4d48-ddc0-08ccd7ffe10f"
      },
      "source": [
        "print(raw_text[:1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alice’s adventures in wonderland\n",
            "\n",
            "lewis carroll\n",
            "\n",
            "the millennium fulcrum edition 3.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "chapter i. down the rabbit-hole\n",
            "\n",
            "alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothing to do: once or twice she had peeped into the\n",
            "book her sister was reading, but it had no pictures or conversations in\n",
            "it, ‘and what is the use of a book,’ thought alice ‘without pictures or\n",
            "conversations?’\n",
            "\n",
            "so she was considering in her own mind (as well as she could, for the\n",
            "hot day made her feel very sleepy and stupid), whether the pleasure\n",
            "of making a daisy-chain would be worth the trouble of getting up and\n",
            "picking the daisies, when suddenly a white rabbit with pink eyes ran\n",
            "close by her.\n",
            "\n",
            "there was nothing so very remarkable in that; nor did alice think it so\n",
            "very much out of the way to hear the rabbit say to itself, ‘oh dear!\n",
            "oh dear! i shall be late!’ (when she thought it over afterwards, it\n",
            "occurred to her that she ought to have wondered at this, but at the time\n",
            "it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19u0XHcTw9VL"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        " - Prepare the data for **modeling** by the **neural network**. \n",
        " \n",
        "- We cannot model the **characters** directly, instead we must convert the **characters** to **integers**.\n",
        "\n",
        " - We can do this **easily** by first creating a set of all of the **distinct** characters in the book, then **creating** a **map** of each **character** to a unique integer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFyFuPSXw9VL"
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxn8F9-0w9VN",
        "outputId": "b5d08d6a-49f5-44b8-e552-c3af6478473d"
      },
      "source": [
        "chars[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '0',\n",
              " '3',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1bq6-Y4w9VQ",
        "outputId": "c88002bc-cc25-4600-ae1b-1490aa8a1e5c"
      },
      "source": [
        "char_to_int"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '(': 3,\n",
              " ')': 4,\n",
              " '*': 5,\n",
              " ',': 6,\n",
              " '-': 7,\n",
              " '.': 8,\n",
              " '0': 9,\n",
              " '3': 10,\n",
              " ':': 11,\n",
              " ';': 12,\n",
              " '?': 13,\n",
              " '[': 14,\n",
              " ']': 15,\n",
              " '_': 16,\n",
              " 'a': 17,\n",
              " 'b': 18,\n",
              " 'c': 19,\n",
              " 'd': 20,\n",
              " 'e': 21,\n",
              " 'f': 22,\n",
              " 'g': 23,\n",
              " 'h': 24,\n",
              " 'i': 25,\n",
              " 'j': 26,\n",
              " 'k': 27,\n",
              " 'l': 28,\n",
              " 'm': 29,\n",
              " 'n': 30,\n",
              " 'o': 31,\n",
              " 'p': 32,\n",
              " 'q': 33,\n",
              " 'r': 34,\n",
              " 's': 35,\n",
              " 't': 36,\n",
              " 'u': 37,\n",
              " 'v': 38,\n",
              " 'w': 39,\n",
              " 'x': 40,\n",
              " 'y': 41,\n",
              " 'z': 42,\n",
              " '‘': 43,\n",
              " '’': 44,\n",
              " '“': 45,\n",
              " '”': 46}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1T7lVcpw9VU"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- You can see that there may be **some characters** that we could remove to further **clean up** the **dataset** that will **reduce** the vocabulary and may improve the **modeling process**.\n",
        "\n",
        "- Now that the book has been loaded and the **mapping prepared**, we can **summarize the dataset**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNxNYjSjw9VV",
        "outputId": "9d46c24b-964f-4dd7-9ca2-e3f53dff87a7"
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Unique Vocab: \", n_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  144435\n",
            "Total Unique Vocab:  47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8yjLYm4w9VY"
      },
      "source": [
        "#### Creating Input and Output variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GXlS9asw9VZ"
      },
      "source": [
        " - We will split the book text up into **subsequences** with a fixed **length** of **100** characters.\n",
        "\n",
        " - Each **training pattern** of the network is comprised of **100** time steps of one character (X) **followed** by one **character** output (y). \n",
        " \n",
        "- When creating these **sequences**, we slide this window along the whole book one **character** at a time, allowing each **character** a chance to be **learned** from the **100 characters** that preceded it.\n",
        "\n",
        " - For example, if the **sequence length** is 5 (for simplicity) then the **first** two **training patterns** would be as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0xuQVsXw9Va",
        "outputId": "6677fbc3-e176-4c10-d1cc-556d13428f02"
      },
      "source": [
        "maxlen = 100\n",
        "step = 1\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(raw_text) - maxlen, step):\n",
        "    sentences.append(raw_text[i: i + maxlen])\n",
        "    next_chars.append(raw_text[i + maxlen])\n",
        "print('Number of sequences:', len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sequences: 144335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91JQrMKAw9Vc",
        "outputId": "a6028a6c-c105-426b-cb10-7047962affaa"
      },
      "source": [
        "sentences[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alice’s adventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\n\\n\\n\\nchapter i. d',\n",
              " 'lice’s adventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\n\\n\\n\\nchapter i. do',\n",
              " 'ice’s adventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\n\\n\\n\\nchapter i. dow',\n",
              " 'ce’s adventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\n\\n\\n\\nchapter i. down',\n",
              " 'e’s adventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\n\\n\\n\\nchapter i. down ',\n",
              " '’s adventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\n\\n\\n\\nchapter i. down t',\n",
              " 's adventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\n\\n\\n\\nchapter i. down th',\n",
              " ' adventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\n\\n\\n\\nchapter i. down the',\n",
              " 'adventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\n\\n\\n\\nchapter i. down the ',\n",
              " 'dventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\n\\n\\n\\nchapter i. down the r']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iERDpcW4w9Vf",
        "outputId": "59ab90d1-e736-4057-b21f-e55c15278d97"
      },
      "source": [
        "next_chars[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['o',\n",
              " 'w',\n",
              " 'n',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " ' ',\n",
              " 'r',\n",
              " 'a',\n",
              " 'b',\n",
              " 'b',\n",
              " 'i',\n",
              " 't',\n",
              " '-',\n",
              " 'h',\n",
              " 'o',\n",
              " 'l',\n",
              " 'e',\n",
              " '\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUxiqddZZuyQ"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- As we split up the **book** into these sequences, we **convert** the **characters** to **integers** using our **lookup** table we prepared earlier.\n",
        "\n",
        "- This will help us **during** the text **generation** step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2-TyCoCZu8R",
        "outputId": "36fe4033-5dce-407d-f85a-e396f4cd19d3"
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  144335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnoC0kkBw9Vj"
      },
      "source": [
        "- **One-hot encode** the **input** sequences, and pack them in a **3D Numpy** array **X** of shape **`(sequences, maxlen, unique_characters)`**.\n",
        "\n",
        "- Prepare an array **y** containing the corresponding **one-hot-encoded characters** that come after each **extracted** sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9PzloY5Xdfb"
      },
      "source": [
        "- Creating **zero** matrices **X** and **y** of the required shapes.\n",
        "\n",
        "- This will help us **easily** create the one-hot encoded version of our **input** and **target** data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ync11nLKRBVd",
        "outputId": "a14951c3-ab72-4690-e032-710b51130e29"
      },
      "source": [
        "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "X[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       ...,\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxADXVwaT0HQ",
        "outputId": "ce63a408-e04e-47a9-ca21-60b34f183a57"
      },
      "source": [
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "y[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iZjacewUAJ1",
        "outputId": "10399834-ade4-4ad6-d6f1-fad2ba1c8e1e"
      },
      "source": [
        "print('Shape of X:', X.shape)\n",
        "print('Shape of y:', y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X: (144335, 100, 47)\n",
            "Shape of y: (144335, 47)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "971wQUAIw9WJ"
      },
      "source": [
        " - To convert the **output** **patterns** (single characters converted to integers) into a **one hot encoding**. \n",
        "\n",
        "- Doing this we can **configure** the network to **predict** the **probability** of each of the **47** different characters in the **vocabulary** rather than trying to **force** it to predict **precisely** the next character.\n",
        "\n",
        "- Each **y** value is converted into a **sparse vector** with a **length** of 47, full of **zeros** except with a 1 in the column for the **letter** (integer) that the pattern represents.\n",
        "\n",
        " - For example, when **`n`** (integer value 31) is one hot **encoded** it looks as follows:\n",
        "\n",
        "   __[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. 0.  0.  0.  0.  0.  0.  0.  0.]__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5S3q5Myw9WM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmMO_dLzScRu"
      },
      "source": [
        "for i, sentence in enumerate(sentences):\n",
        "    \n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_to_int[char]] = 1\n",
        "    \n",
        "    y[i, char_to_int[next_chars[i]]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7huXBh9UVIBE",
        "outputId": "40e723fd-f068-488f-ee7b-d3fc43ade72b"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       ...,\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False,  True, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oENhQf0VOd9",
        "outputId": "d0db9a83-5c17-4873-f01c-5024c6948fcc"
      },
      "source": [
        "y[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False,  True, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdxgqiTOQUiH"
      },
      "source": [
        "---\n",
        "<a name = Section6></a>\n",
        "# **6. Machine Translation Model with Attention Mechanism**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbY5T9yew9U0"
      },
      "source": [
        "- We'll be using the following **process sequence** in this notebook:\n",
        "\n",
        "<br>   \n",
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/char_lstm_flow0.png\"width=\"700\"height=\"400\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xmDsdjapjaL"
      },
      "source": [
        "<a name = Section11></a>\n",
        "### **6.1 Define LSTM model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO8xo2vJw9Wd"
      },
      "source": [
        "- Define our LSTM model:\n",
        "\n",
        " - Define a single hidden **LSTM** layer with **256** memory units.\n",
        "\n",
        " - The network uses **dropout** with a probability of **20 Percent**. \n",
        "\n",
        " - The **output layer** is a Dense layer using the softmax activation function to output a probability **prediction** for each of the **47** characters between 0 and 1.\n",
        "\n",
        "- The problem is really a single character **classification** problem with 47 classes and as such is defined as **optimizing** the **log loss** (cross entropy), here using the **ADAM optimization** algorithm for **speed**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9H8FSuow9We"
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvk9Nh2yw9Wg"
      },
      "source": [
        "- We are **modeling** the entire **training** dataset to learn the **probability** of each character in a **sequence**.\n",
        "\n",
        "- This would be a model that **predicts** each **character** in the training dataset **perfectly**. \n",
        "\n",
        "- Instead we are interested in a **generalization** of the dataset that **minimizes** the chosen loss function. \n",
        "\n",
        "- We are **seeking** a balance between **generalization** and **overfitting** but short of **memorization**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQildT8mw9Wg"
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAzNCg1DpjAs"
      },
      "source": [
        "<a name = Section11></a>\n",
        "### **6.2 Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKeEQ9f5w9Wo",
        "outputId": "d4f98a65-1017-4112-bad1-07ee2315b277"
      },
      "source": [
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1128/1128 [==============================] - 29s 19ms/step - loss: 2.7670\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.45730, saving model to weights-improvement-01-2.4573.hdf5\n",
            "Epoch 2/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 2.0148\n",
            "\n",
            "Epoch 00002: loss improved from 2.45730 to 1.95006, saving model to weights-improvement-02-1.9501.hdf5\n",
            "Epoch 3/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 1.7811\n",
            "\n",
            "Epoch 00003: loss improved from 1.95006 to 1.74840, saving model to weights-improvement-03-1.7484.hdf5\n",
            "Epoch 4/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 1.6257\n",
            "\n",
            "Epoch 00004: loss improved from 1.74840 to 1.61461, saving model to weights-improvement-04-1.6146.hdf5\n",
            "Epoch 5/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 1.5388\n",
            "\n",
            "Epoch 00005: loss improved from 1.61461 to 1.52086, saving model to weights-improvement-05-1.5209.hdf5\n",
            "Epoch 6/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 1.4450\n",
            "\n",
            "Epoch 00006: loss improved from 1.52086 to 1.44502, saving model to weights-improvement-06-1.4450.hdf5\n",
            "Epoch 7/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 1.3811\n",
            "\n",
            "Epoch 00007: loss improved from 1.44502 to 1.38157, saving model to weights-improvement-07-1.3816.hdf5\n",
            "Epoch 8/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 1.3119\n",
            "\n",
            "Epoch 00008: loss improved from 1.38157 to 1.32408, saving model to weights-improvement-08-1.3241.hdf5\n",
            "Epoch 9/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 1.2710\n",
            "\n",
            "Epoch 00009: loss improved from 1.32408 to 1.27543, saving model to weights-improvement-09-1.2754.hdf5\n",
            "Epoch 10/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 1.2269\n",
            "\n",
            "Epoch 00010: loss improved from 1.27543 to 1.23184, saving model to weights-improvement-10-1.2318.hdf5\n",
            "Epoch 11/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 1.1829\n",
            "\n",
            "Epoch 00011: loss improved from 1.23184 to 1.19224, saving model to weights-improvement-11-1.1922.hdf5\n",
            "Epoch 12/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 1.1418\n",
            "\n",
            "Epoch 00012: loss improved from 1.19224 to 1.15508, saving model to weights-improvement-12-1.1551.hdf5\n",
            "Epoch 13/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 1.1035\n",
            "\n",
            "Epoch 00013: loss improved from 1.15508 to 1.11600, saving model to weights-improvement-13-1.1160.hdf5\n",
            "Epoch 14/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 1.0706\n",
            "\n",
            "Epoch 00014: loss improved from 1.11600 to 1.08472, saving model to weights-improvement-14-1.0847.hdf5\n",
            "Epoch 15/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 1.0354\n",
            "\n",
            "Epoch 00015: loss improved from 1.08472 to 1.05293, saving model to weights-improvement-15-1.0529.hdf5\n",
            "Epoch 16/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 0.9969\n",
            "\n",
            "Epoch 00016: loss improved from 1.05293 to 1.02381, saving model to weights-improvement-16-1.0238.hdf5\n",
            "Epoch 17/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 0.9803\n",
            "\n",
            "Epoch 00017: loss improved from 1.02381 to 0.99806, saving model to weights-improvement-17-0.9981.hdf5\n",
            "Epoch 18/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 0.9556\n",
            "\n",
            "Epoch 00018: loss improved from 0.99806 to 0.97297, saving model to weights-improvement-18-0.9730.hdf5\n",
            "Epoch 19/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 0.9232\n",
            "\n",
            "Epoch 00019: loss improved from 0.97297 to 0.95111, saving model to weights-improvement-19-0.9511.hdf5\n",
            "Epoch 20/20\n",
            "1128/1128 [==============================] - 22s 19ms/step - loss: 0.9057\n",
            "\n",
            "Epoch 00020: loss improved from 0.95111 to 0.92846, saving model to weights-improvement-20-0.9285.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7f702e2390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD48Lg8zw9Ws"
      },
      "source": [
        " **Observation:**\n",
        "\n",
        " - In our case, the file **weights-improvement-20-0.9285.hdf5** has the **least loss** value of **0.9285**\n",
        "\n",
        " - This file was generated in the **last** (**20th**) epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikIp-KyQw9Wq",
        "outputId": "73de5adc-da35-4676-faa4-d3c873f12e7c"
      },
      "source": [
        "!ls # to see list of all weight checkpoint files created"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\t\t\t    weights-improvement-11-1.1922.hdf5\n",
            "weights-improvement-01-2.4573.hdf5  weights-improvement-12-1.1551.hdf5\n",
            "weights-improvement-02-1.9501.hdf5  weights-improvement-13-1.1160.hdf5\n",
            "weights-improvement-03-1.7484.hdf5  weights-improvement-14-1.0847.hdf5\n",
            "weights-improvement-04-1.6146.hdf5  weights-improvement-15-1.0529.hdf5\n",
            "weights-improvement-05-1.5209.hdf5  weights-improvement-16-1.0238.hdf5\n",
            "weights-improvement-06-1.4450.hdf5  weights-improvement-17-0.9981.hdf5\n",
            "weights-improvement-07-1.3816.hdf5  weights-improvement-18-0.9730.hdf5\n",
            "weights-improvement-08-1.3241.hdf5  weights-improvement-19-0.9511.hdf5\n",
            "weights-improvement-09-1.2754.hdf5  weights-improvement-20-0.9285.hdf5\n",
            "weights-improvement-10-1.2318.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "batVB69zw9Wz"
      },
      "source": [
        "- **Generating** text using the **trained** LSTM model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kv_sbTlw9W4"
      },
      "source": [
        "# load the network weights\n",
        "filename = \"weights-improvement-20-0.9285.hdf5\" # use the weight checkpoint file that has least loss value.\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYGBuEtCw9W7"
      },
      "source": [
        " - Also, when **preparing** the mapping of **unique** characters to integers, we must also **create** a reverse **mapping** that we can use to **convert** the integers back to **characters** so that we can understand the **predictions**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQhIsXmQw9W8"
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKxb6QFApi4G"
      },
      "source": [
        "<a name = Section11></a>\n",
        "### **6.3 Model Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTxByqDbw9W_"
      },
      "source": [
        "The simplest way to use the Keras **LSTM model** to make predictions is to:\n",
        "\n",
        " - First start off with a **seed sequence** as input, generate the next character then update the seed sequence to add the **generated** character on the end and trim off the **first** character.\n",
        " \n",
        " <br>  \n",
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/ytr.PNG\"width=\"400\" height=\"500\" /></center>\n",
        "\n",
        "<br>  \n",
        " - This process is **repeated** for as long as we want to **predict** new characters (e.g. a sequence of 1,000 characters in length).\n",
        "\n",
        "- We can pick a **random** input pattern as our **seed sequence**, then print generated characters as we **generate** them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS5xPMU1w9XC",
        "outputId": "2703280c-045d-4789-aaaa-e363fcfe952c"
      },
      "source": [
        "import sys\n",
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX) - 1)\n",
        "pattern = dataX[start]\n",
        "generated_text = ''.join([int_to_char[value] for value in pattern])\n",
        "\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", generated_text, \"\\\"\")\n",
        "\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "    sampled = np.zeros((1, maxlen, len(chars)))\n",
        "    \n",
        "    for t, char in enumerate(generated_text):\n",
        "        sampled[0, t, char_to_int[char]] = 1.\n",
        "\n",
        "    prediction = model.predict(sampled, verbose=0)\n",
        "\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" ng as you’re falling\n",
            "through the air! do you think you could manage it?) ‘and what an\n",
            "ignorant littl \"\n",
            "eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyReTYA-w9XA"
      },
      "source": [
        "Running the below code first outputs the selected random seed, then each character as it is generated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdF2cotFw9XG"
      },
      "source": [
        "We can note some observations about the generated text.\n",
        "\n",
        " - It generally **conforms to the line format** observed in the original text of less than 80 characters before a new line.\n",
        "\n",
        " - The **characters** are separated into word-like groups and most groups are actual English words (e.g. **`the`**, **`little`** and **`was`**), but many do not (e.g. **`lott`**, **`tiie`** and **`taede`**).\n",
        "\n",
        " - Some of the **words** in sequence make **sense**(e.g. **`and the white rabbit`**), but many do not (e.g. **`wese tilel`**).\n",
        "\n",
        " - The fact that this character **based** model of the book **produces** output like this is very **impressive**. It gives you a sense of the learning **capabilities** of **LSTM** networks.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvTmZXPzpiwO"
      },
      "source": [
        "<a name = Section11></a>\n",
        "### **6.4 Building Larger Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBLiY3RVw9XI"
      },
      "source": [
        "![](https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/char_lstm_flow7.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMQsx7Vnw9XJ"
      },
      "source": [
        "- Keep the number of **memory** units the same at **256**, but add a second layer.\n",
        "\n",
        "- **Increase** the number of **training** epochs from 20 to 50 and **decrease** the batch size from **128 to 64** to give the network more of an **opportunity** to be updated and learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V70P8sAiw9XJ",
        "outputId": "2cadae67-b900-442b-98e9-fdb0c631668d"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "144335/144335 [==============================] - 615s 4ms/step - loss: 2.7930\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.79302, saving model to weights-improvement-01-2.7930-bigger.hdf5\n",
            "Epoch 2/50\n",
            "144335/144335 [==============================] - 608s 4ms/step - loss: 2.4019\n",
            "\n",
            "Epoch 00002: loss improved from 2.79302 to 2.40192, saving model to weights-improvement-02-2.4019-bigger.hdf5\n",
            "Epoch 3/50\n",
            "144335/144335 [==============================] - 607s 4ms/step - loss: 2.1985\n",
            "\n",
            "Epoch 00003: loss improved from 2.40192 to 2.19847, saving model to weights-improvement-03-2.1985-bigger.hdf5\n",
            "Epoch 4/50\n",
            "144335/144335 [==============================] - 606s 4ms/step - loss: 2.0658\n",
            "\n",
            "Epoch 00004: loss improved from 2.19847 to 2.06576, saving model to weights-improvement-04-2.0658-bigger.hdf5\n",
            "Epoch 5/50\n",
            "144335/144335 [==============================] - 604s 4ms/step - loss: 1.9743\n",
            "\n",
            "Epoch 00005: loss improved from 2.06576 to 1.97430, saving model to weights-improvement-05-1.9743-bigger.hdf5\n",
            "Epoch 6/50\n",
            "   192/144335 [..............................] - ETA: 10:13 - loss: 2.0523"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-1a1bf2020bc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SxTp8wTw9XL"
      },
      "source": [
        "ls # to see list of all weight checkpoint files created"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eDLqNKzpipV"
      },
      "source": [
        "<a name = Section11></a>\n",
        "### **6.5 Text generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fQbpYZ5w9XP"
      },
      "source": [
        "![](https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/char_lstm_flow8.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK51_QUIw9XP"
      },
      "source": [
        "filename = \"weights-improvement-47-1.2219-bigger.hdf5\" # use the weight checkpoint file that has least loss value.\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print \"Seed:\"\n",
        "print \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\"\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print \"\\nDone."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13zmsfALcd0j"
      },
      "source": [
        "----\n",
        "\n",
        "<a id=section7></a>\n",
        "# **7. Conclusion**\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfG0oYB4w9Xb"
      },
      "source": [
        " - We can see that **generally** there are **fewer spelling mistakes** and the text looks more **realistic**, but is still quite **nonsensical**.\n",
        "\n",
        "- If we have **more** data, a **bigger** model, and train longer, we may get more **interesting results**.\n",
        "\n",
        "- However, to get a **very** interesting results, we should **instead** use **Long Short-Term Memory** (LSTM) model with more than **one layer deep**.\n",
        "\n",
        "- LSTM models **outperform** simple RNN due to its **ability** in capturing **longer time** dependencies.\n",
        "\n",
        "- We can control the level of **randomness** using the **sampling strategy**. Here, we **balanced** between what the **model thinks** it’s the right character and the level of randomness."
      ]
    }
  ]
}